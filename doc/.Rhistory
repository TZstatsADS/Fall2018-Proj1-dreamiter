type(hm_data)
typeof(hm_data)
str(hm_data)
dtm <- VCorpus(VectorSource(hm_data$text)) %>% DocumentTermMatrix()
#convert rownames to filenames#convert rownames to filenames
#rownames(dtm) <- paste(corpus.list$type, corpus.list$File, corpus.list$Term, corpus.list$sent.id, sep="_")
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm <- VCorpus(VectorSource(hm_data$text[1:1000])) %>% DocumentTermMatrix()
#convert rownames to filenames#convert rownames to filenames
#rownames(dtm) <- paste(corpus.list$type, corpus.list$File, corpus.list$Term, corpus.list$sent.id, sep="_")
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm  <- dtm[rowTotals> 0, ]
#corpus.list=corpus.list[rowTotals>0, ]
#Set parameters for Gibbs sampling
burnin <- 400
iter <- 200
thin <- 50
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 10
ap_lda <- LDA(dtm, k = 5, control = list(seed = 1234))
#Run LDA using Gibbs sampling
ldaOut <- LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("../out/LDAGibbs",k,"DocsToTopics.csv"))
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(wordcloud)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(sentimentr)
library(dplyr)
library(qdap)
library(syuzhet)
library(ggplot2)
library(topicmodels)
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
baseform_hm,
num_sentence,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount))
sum(is.na(hm_data$text))
dtm <- VCorpus(VectorSource(hm_data$text)) %>% DocumentTermMatrix()
#convert rownames to filenames#convert rownames to filenames
#rownames(dtm) <- paste(corpus.list$type, corpus.list$File, corpus.list$Term, corpus.list$sent.id, sep="_")
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm <- VCorpus(VectorSource(hm_data$text[1:1000])) %>% DocumentTermMatrix()
#convert rownames to filenames#convert rownames to filenames
#rownames(dtm) <- paste(corpus.list$type, corpus.list$File, corpus.list$Term, corpus.list$sent.id, sep="_")
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm  <- dtm[rowTotals> 0, ]
#corpus.list=corpus.list[rowTotals>0, ]
#Set parameters for Gibbs sampling
burnin <- 400
iter <- 200
thin <- 50
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 10
ap_lda <- LDA(dtm, k = 5, control = list(seed = 1234))
#Run LDA using Gibbs sampling
ldaOut <- LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("../out/LDAGibbs",k,"DocsToTopics.csv"))
ldaOut.terms <- as.matrix(terms(ldaOut,20))
topicProbabilities <- as.data.frame(ldaOut@gamma)
head(ldaOut.topics)
head(ldaOut.terms)
head(topicProbabilities)
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
Sys.setenv("R_MAX_VSIZE" = 8e9)
dtm <- VCorpus(VectorSource(hm_data$text)) %>% DocumentTermMatrix()
#convert rownames to filenames#convert rownames to filenames
#rownames(dtm) <- paste(corpus.list$type, corpus.list$File, corpus.list$Term, corpus.list$sent.id, sep="_")
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
Sys.setenv("R_MAX_VSIZE" = 8e15)
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm <- VCorpus(VectorSource(hm_data$text[!is.na(hm_data$text),])) %>% DocumentTermMatrix()
dtm <- VCorpus(VectorSource(hm_data$text[!is.na(hm_data$text)])) %>% DocumentTermMatrix()
#convert rownames to filenames#convert rownames to filenames
#rownames(dtm) <- paste(corpus.list$type, corpus.list$File, corpus.list$Term, corpus.list$sent.id, sep="_")
#rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
#dtm  <- dtm[rowTotals> 0, ]
#corpus.list=corpus.list[rowTotals>0, ]
#Set parameters for Gibbs sampling
burnin <- 400
iter <- 200
thin <- 50
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 10
#Run LDA using Gibbs sampling
ldaOut <- LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
View(dtm)
nrow(dtm)
rowTotals <- apply(dtm, 1, sum) #Find the sum of words in each Document
str(hm_data)
table(hm_data$count)
dtm <- VCorpus(VectorSource(hm_data$text[1:3000])) %>% DocumentTermMatrix()
#convert rownames to filenames#convert rownames to filenames
#rownames(dtm) <- paste(corpus.list$type, corpus.list$File, corpus.list$Term, corpus.list$sent.id, sep="_")
rowTotals <- apply(dtm, 1, sum) #Find the sum of words in each Document
dtm  <- dtm[rowTotals> 0, ]
#corpus.list=corpus.list[rowTotals>0, ]
#Set parameters for Gibbs sampling
burnin <- 400
iter <- 200
thin <- 50
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 10
#Run LDA using Gibbs sampling
ldaOut <- LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
?apply
View(hm_data)
lengt(hm_data$text) == 0
sum(length(hm_data$text) == 0)
sum(length(hm_data$text) == 1)
sum(length(hm_data$text) == 2)
sum(nchar(hm_data$text) == 0)
sum(nchar(hm_data$text) == 1)
dtm <- VCorpus(VectorSource(hm_data$text)) %>% DocumentTermMatrix()
#convert rownames to filenames#convert rownames to filenames
#rownames(dtm) <- paste(corpus.list$type, corpus.list$File, corpus.list$Term, corpus.list$sent.id, sep="_")
rowTotals <- apply(dtm, 1, sum) #Find the sum of words in each Document
Sys.setenv('R_MAX_VSIZE'=32000000000)
rowTotals <- apply(dtm, 1, sum) #Find the sum of words in each Document
Sys.setenv('R_MAX_VSIZE'=32000000000000000)
rowTotals <- apply(dtm, 1, sum) #Find the sum of words in each Document
test.num <- NULL
for (i in 1:nrow(dtm)){
test.num[i] <- sum(dtm[i,])
}
dtm <- dtm[test.num>0,]
sum(test.num) == 0
test.num[1]
which(test.num == 0)
#Set parameters for Gibbs sampling
burnin <- 400
iter <- 200
thin <- 50
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 10
#Run LDA using Gibbs sampling
ldaOut <- LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
topicProbabilities
rowTotals <- slam::row_sums(dtm)
dtm <- dtm[rowTotals > 0, ]
#Set parameters for Gibbs sampling
burnin <- 800
iter <- 400
thin <- 100
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 10
#Run LDA using Gibbs sampling
ldaOut <- LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
?wordcloud
library(plyr)
# By Predicted Category
ddply(word_count_by_category, .(predicted_category), wordcloud_group)
packages.used=c("plyr","tm","tidytext","tidyverse","DT","wordcloud","scales","wordcloud2",
"ngram","sentimentr","dplyr","qdap","syuzhet","ggplot2","topicmodels")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(wordcloud)
library(scales)
library(wordcloud2)
library(ngram)
library(sentimentr)
library(dplyr)
library(qdap)
library(syuzhet)
library(ggplot2)
library(topicmodels)
# Function
source("../lib/wordcloud_group.R")
library(plyr)
# By Predicted Category
ddply(word_count_by_category, .(predicted_category), wordcloud_group)
packages.used=c("plyr","tm","tidytext","tidyverse","DT","wordcloud","scales","wordcloud2",
"ngram","sentimentr","dplyr","qdap","syuzhet","ggplot2","topicmodels")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(wordcloud)
library(scales)
library(wordcloud2)
library(ngram)
library(sentimentr)
library(dplyr)
library(qdap)
library(syuzhet)
library(ggplot2)
library(topicmodels)
# Function
source("../lib/wordcloud_group.R")
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
baseform_hm,
num_sentence,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount))
bag_of_words <-  hm_data %>%
unnest_tokens(word, text)
word_count <- bag_of_words %>%
count(word, sort = TRUE)
knitr::opts_chunk$set(echo = TRUE)
packages.used=c("plyr","tm","tidytext","tidyverse","DT","wordcloud","scales","wordcloud2",
"ngram","sentimentr","dplyr","qdap","syuzhet","ggplot2","topicmodels")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(wordcloud)
library(scales)
library(wordcloud2)
library(ngram)
library(sentimentr)
library(dplyr)
library(qdap)
library(syuzhet)
library(ggplot2)
library(topicmodels)
# Function
source("../lib/wordcloud_group.R")
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
baseform_hm,
num_sentence,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount))
bag_of_words <-  hm_data %>%
unnest_tokens(word, text)
word_count <- bag_of_words %>%
count(word, sort = TRUE)
word_count_by_category <- bag_of_words %>%
group_by(predicted_category) %>%
count(word, sort = TRUE)
word_count_by_gender <- bag_of_words %>%
group_by(gender) %>%
count(word, sort = TRUE)
word_count_by_marital <- bag_of_words %>%
group_by(marital) %>%
count(word, sort = TRUE)
word_count_by_reflection <- bag_of_words %>%
group_by(reflection_period) %>%
count(word, sort = TRUE)
wordcloud(words = word_count$word, freq = word_count$n, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8,"Dark2"))
library(plyr)
# By Predicted Category
ddply(word_count_by_category, .(predicted_category), wordcloud_group)
word_count_by_category %>%
slice(1:10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n, color = predicted_category)) + geom_col() +  facet_wrap(~predicted_category, scales = "free") + xlab(NULL) +
ylab("Word Frequency")+ coord_flip()
# By Gender
ddply(word_count_by_gender[!is.na(word_count_by_gender$gender),], .(gender), wordcloud_group)
word_count_by_gender[!is.na(word_count_by_gender$gender),] %>%
slice(1:10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n, color = gender)) + geom_col() +  facet_wrap(~gender, scales = "free") + xlab(NULL) + ylab("Word Frequency")+ coord_flip()
# By Marital
ddply(word_count_by_marital[!is.na(word_count_by_marital$marital),], .(marital), wordcloud_group)
word_count_by_marital[!is.na(word_count_by_marital$marital),] %>%
slice(1:10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n, color = marital)) + geom_col() +  facet_wrap(~marital, scales = "free") + xlab(NULL) + ylab("Word Frequency")+ coord_flip()
# By Reflection Period
ddply(word_count_by_reflection[!is.na(word_count_by_reflection$reflection_period),], .(reflection_period), wordcloud_group)
word_count_by_reflection[!is.na(word_count_by_reflection$reflection_period),] %>%
slice(1:10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n, color = reflection_period)) + geom_col() +  facet_wrap(~reflection_period, scales = "free") + xlab(NULL) + ylab("Word Frequency")+ coord_flip()
hm_data$Sentiment.Value <- get_sentiment(hm_data$text)
ggplot(hm_data[(!is.na(hm_data$gender))&&(!is.na(hm_data$marital)),], aes(x = gender, y = Sentiment.Value, color = marital)) + geom_boxplot()
country.int <- tail(names(sort(table(hm_data$country))), 10)
ggplot(subset(hm_data, country %in% country.int), aes(x = country, y = Sentiment.Value, color = country)) + geom_boxplot()
hm_data$age <- as.integer(hm_data$age)
hm_data$age.interval <- findInterval(hm_data$age, seq(0,240,10))
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot()
ggplot(hm_data[(!is.na(hm_data$gender))&(!is.na(hm_data$marital)),], aes(x = gender, y = Sentiment.Value, color = marital)) + geom_boxplot()
hm_data$Sentiment.Value <- get_sentiment(hm_data$text)
ggplot(hm_data[(!is.na(hm_data$gender))&(!is.na(hm_data$marital)),], aes(x = gender, y = Sentiment.Value, color = marital)) + geom_boxplot()
country.int <- tail(names(sort(table(hm_data$country))), 10)
ggplot(subset(hm_data, country %in% country.int), aes(x = country, y = Sentiment.Value, color = country)) + geom_boxplot()
hm_data$age <- as.integer(hm_data$age)
hm_data$age.interval <- findInterval(hm_data$age, seq(0,240,10))
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot()
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_y_continuous(limits = c(0,250))
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(limits = c(0,250))
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_discrete(labels=c(0,250))
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_discrete(labels=seq(0,240,10))
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(labels=seq(0,240,10))
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(labels=seq(0,240,10))
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot()
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(labels=c("5"="50"))
hm_data$age <- as.integer(hm_data$age)
x.interval <- seq(0,240,10)
hm_data$age.interval <- findInterval(hm_data$age, x.interval)
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(labels=as.character(x.interval))
hm_data$age <- as.integer(hm_data$age)
x.interval <- seq(0,250,10)
hm_data$age.interval <- findInterval(hm_data$age, x.interval)
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(labels=as.character(x.interval))
hm_data$age <- as.integer(hm_data$age)
x.interval <- seq(0,250,10)
hm_data$age.interval <- findInterval(hm_data$age, x.interval)
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot()
hm_data$age <- as.integer(hm_data$age)
x.interval <- seq(0,250,10)
xx.interval <- seq(0,250,50)
hm_data$age.interval <- findInterval(hm_data$age, x.interval)
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(labels = as.character(xx.interval))
# Sentiment Value vs. Gender & Marital Status
ggplot(hm_data[(!is.na(hm_data$gender))&(!is.na(hm_data$marital)),], aes(x = gender, y = Sentiment.Value, color = marital)) + geom_boxplot()
# Sentiment Value vs. 10 Countries with the most word entries
country.int <- tail(names(sort(table(hm_data$country))), 10)
ggplot(subset(hm_data, country %in% country.int), aes(x = country, y = Sentiment.Value, color = country)) + geom_boxplot()
# Sentiment Value vs. Age groups
hm_data$age <- as.integer(hm_data$age)
x.interval <- seq(0,250,10)
xx.interval <- seq(0,250,50)
hm_data$age.interval <- findInterval(hm_data$age, x.interval)
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(labels = as.character(xx.interval))
table(hm_data$country)
table(hm_data$age)
table(hm_data$age.interval)
View(hm_data)
View(demo_data)
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(labels = as.character(xx.interval), limits = c(0,100))
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(limits = c(0,100))
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(limits = c(0,10))
# Sentiment Value vs. Age groups
hm_data$age <- as.integer(hm_data$age)
x.interval <- seq(0,250,10)
xx.interval <- seq(0,100,25)
hm_data$age.interval <- findInterval(hm_data$age, x.interval)
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(labels = as.character(xx.interval), limits = c(0,10))
# Sentiment Value vs. Age groups
hm_data$age <- as.integer(hm_data$age)
x.interval <- seq(0,250,10)
xx.interval <- seq(0,100,25)
hm_data$age.interval <- findInterval(hm_data$age, x.interval)
ggplot(hm_data[!is.na(hm_data$age),], aes(x = age.interval, y = Sentiment.Value, group = age.interval)) + geom_boxplot() + scale_x_continuous(labels = as.character(xx.interval), limits = c(0,10))
